{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Begin\n",
    "\n",
    "This is a practical part of your ASR-TTS course. In total you will have 5 labs. Three of which will be focused on Automatic Speech Recognition and two on Text-to-Speech models. Each lab will last two hours and consist of two parts:\n",
    "* Reading Part\n",
    "* Coding Part \n",
    "\n",
    "In each part you might find question or tasks/activities to complete. The grading of the labs is explained below.\n",
    "\n",
    "LAB 2/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will you learn in LAB 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How feature extraction works in practice, including time--frequency representations and their parameters.\n",
    "* How different decoder strategies (e.g. greedy decoding. beam search...) affect ASR outputs.\n",
    "* How ASR errors can be analyzed beyond a single score using detailed WER breakdowns.\n",
    "* How tensor shapes propagate through an ASR pipeline (waveforms, features, batches).\n",
    "* How to identify and fix common tensor shape and data type errors in ASR systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition: Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Read the following, remember and explain how feature extraction works: \n",
    "\n",
    "https://thesai.org/Downloads/Volume12No8/Paper_21-Automatic_Speech_Recognition_Features_Extraction_Techniques.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw audio is a 1D waveform: amplitude over time. But ASR models usually don’t work directly on raw samples; they rely on features that make speech patterns easier to learn. Today we’ll build the most common ones: spectrograms, log-mel spectrograms, and MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An audio waveform is the most basic representation of sound in a computer.\n",
    "When we record speech, the microphone converts air pressure changes into an electrical signal, and the computer stores this signal as a sequence of numbers over time.\n",
    "\n",
    "Each number represents: how much the air pressure deviates from silence at a specific moment in time\n",
    "\n",
    "So a waveform is simply: amplitude (loudness) as a function of time\n",
    "\n",
    "Unlike humans, a computer does not listen or understand meaning.\n",
    "It only sees numbers. A waveform looks like this to a model:\n",
    "\n",
    "[0.002, 0.01, -0.03, -0.01, 0.0, 0.02, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Rate\n",
    "\n",
    "Speech is stored by sampling the signal many times per second. Example: 16 kHz = 16,000 samples per second every second of speech → 16,000 numbers. Higher sampling rate means more detail, larger files and more computation. Most ASR systems use 16 kHz because it captures speech frequencies well while remaining efficient. While waveforms contain all information, they are not convenient for learning speech patterns directly: \n",
    "\n",
    "* speech information is spread over time\n",
    "* frequency content (pitch, formants) is hidden\n",
    "* small shifts in time change the signal a lot\n",
    "\n",
    "That’s why ASR systems transform waveforms into time–frequency features, such as spectrograms, Mel spectrograms, MFCCs.\n",
    "\n",
    "➡️ This is the goal of feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the code from Lab 1 load one audio file and plot a waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Time Fourier Transform (STFT) Spectrogram — Linear Frequency Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech changes very quickly over time, so instead of analyzing the whole sound at once, we analyze it little by little. The audio signal is first cut into short pieces called frames, each lasting only a few milliseconds (typically about 25 ms), which is short enough that speech is almost stable inside each frame. These frames overlap slightly so that no information is lost at the borders and transitions remain smooth. Before analyzing each frame, we apply a window function, most commonly the Hann window, which gently fades the signal at the beginning and end of the frame to avoid artificial frequencies caused by sharp cuts. Once a frame is short, overlapping, and smoothly windowed, the computer analyzes which frequencies are present and how strong they are. By stacking the frequency information from all frames over time, we obtain a spectrogram, where:\n",
    "* the horizontal axis represents time,\n",
    "* the vertical axis represents frequency (evenly spaced in Hz, called linear frequency),\n",
    "* and the color represents energy.\n",
    "\n",
    "This process is called the Short-Time Fourier Transform (STFT) because we transform short pieces of sound into frequencies, revealing how speech frequencies evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STFT Transformation\n",
    "\n",
    "win_length = 400        \n",
    "hop_length = 160       \n",
    "n_fft = 512             \n",
    "\n",
    "# Create STFT transform\n",
    "stft = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    window_fn=torch.hann_window,\n",
    "    power=2.0            \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_PATH = \"/home/aine/Teaching/ASR-TTS-Course/asr-dataset/speech/61-70968-0000.wav\"  # TODO\n",
    "\n",
    "waveform, sr = torchaudio.load(WAV_PATH)   # waveform: [channels, time]\n",
    "print(\"waveform shape:\", waveform.shape)\n",
    "print(\"sample rate:\", sr)\n",
    "\n",
    "\n",
    "def plot_spectrogram(spec, title=\"Spectrogram\", ylabel=\"Frequency bins\"):\n",
    "    # spec expected shape: [freq, time]\n",
    "    spec_db = 10 * torch.log10(spec + 1e-10)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spec_db.detach().cpu(), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.colorbar(label=\"dB\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "win_length = n_fft\n",
    "\n",
    "spec_transform = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    window_fn=torch.hann_window,\n",
    "    power=2.0,  # power spectrogram\n",
    ")\n",
    "\n",
    "spec = spec_transform(waveform)  # [channel, freq, time]\n",
    "print(\"spectrogram shape:\", spec.shape)\n",
    "\n",
    "plot_spectrogram(spec[0], title=f\"Linear Spectrogram (n_fft={n_fft}, hop={hop_length})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition: Decoders\n",
    "\n",
    "**Name**:\n",
    "\n",
    "The first lab introduced the building blocks of an ASR system, including feature extraction and classification with an acoustic model (wav2vec2), which produced an *emission* matrix (probability for each character at each time step). From this emission matrix, we could compute the most likely character at each time step using a naïve *greedy* decoder. The drawback of such an approach is the lack of context, which can produce sequences of characters that do not correspond to actual words, and/or sequences of words that are incorrect / do not correspond to any language rules.\n",
    "\n",
    "In this lab, we introduce and compare multiple decoding techniques. One of the more advanced techniques is based on [connectionist temporal classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c) (CTC). The general idea of such a decoder is to consider some context (sequences of characters, possible words, and possible sequences of words), in oder to yield more likely / realistic outputs than those given by the greedy decoder.\n",
    "\n",
    "<center><a href=\"https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XbIp4pn38rxL_DwzSeYVxQ.png\" width=\"400\"></a></center>\n",
    "\n",
    "To do so, the CTC decoder relies on three main components:\n",
    "    \n",
    "- A **beam search**, which is an algorithm to efficently find the *best path* from the emission matrix, that is, the sequence of characters with highest probability (rather than the sequence of individually most likely characters).\n",
    "- A **lexicon**, which is a mapping between token sequences (list of characters) and words. It is used to restrict the search space of the decoder to words that only belong to this dictionary (e.g., the word \"azfpojazflj\" does not exist in the English vocabulary).\n",
    "- A **language model**, which specifies sequences of words that are more likely to occur than others. A common choice of language model is an $n$-gram, which is a statistical model for the probability of occurrence of a given word based on the previous $n$ ones (for instance, the sequence \"the sky is\" is more likely to be followed with \"blue\" rather than \"trumpet\").\n",
    "\n",
    "The CTC decoder combines these ingredients to compute the score of several word sequences (or *hypothesis*), in order to find the best possible transcript. In this lab, we study the influence of the lexicon, language model, and the beam search size onto ASR performance from a practical perspective, without going into the technical details of the [beam search algorithm](https://www.width.ai/post/what-is-beam-search) or the [CTC loss](https://distill.pub/2017/ctc/), which can also be used for training the network (as we will see in lab 3).\n",
    "\n",
    "**Note**: This lab is based on this [tutorial](https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html), which you can check for more details on CTC decoder parameters in torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.models.decoder import ctc_decoder, download_pretrained_files\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "torch.random.manual_seed(0)\n",
    "MAX_FILES = 2 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "As in the previous lab, we first load an example speech signal, and we display it. We also provide the function to get the true transcript and compute the WER. Finally, we load the wav2vec2 acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the function for loading the true transcript and computing the WER\n",
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().split()\n",
    "    return true_transcript\n",
    "\n",
    "def get_wer(true_transcript, est_transcript):\n",
    "    wer = torchaudio.functional.edit_distance(true_transcript, est_transcript) / len(true_transcript)\n",
    "    return wer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "# Apply the model to the waveform to get the emission tensor\n",
    "with torch.inference_mode():\n",
    "    emission, _ = acoustic_model(waveform)\n",
    "    num_time_steps = emission.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Decoder\n",
    "\n",
    "The CTC decoder can be constructed directly by using the `ctc_decoder` function in torchaudio. In addition to the parameters related to the beam search (we will study them later on), it takes as inputs:\n",
    "- the list of tokens, in order to map emissions to characters in the classifier.\n",
    "- the path to the lexicon, expected as a .txt file containing, on each line, a word followed by its space-split tokens (and special end-of-sequence token `|`):\n",
    "\n",
    "```\n",
    "# lexicon.txt\n",
    "a     a |\n",
    "able  a b l e |\n",
    "about a b o u t |\n",
    "...\n",
    "```\n",
    "- the path to the language model, expected as a .bin file.\n",
    "\n",
    "All these are assembled in pretrained files that can be downloaded using the `download_pretrained_files` function (this might take some time as the language model can be large), and then used to contruct the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the files corresponding to (pretrained) language model, which comes with lexicon and tokens\n",
    "files = download_pretrained_files(\"librispeech\")\n",
    "path_lm_tokens = files.tokens\n",
    "path_lm_lexicon = files.lexicon\n",
    "path_lm = files.lm\n",
    "\n",
    "print(path_lm_tokens)\n",
    "print(path_lm_lexicon)\n",
    "print(path_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the first 10 tokens (includes the blank and end-of-word token)\n",
    "with open(path_lm_tokens, 'r') as f:\n",
    "    tok = f.read().splitlines()\n",
    "print(\"\\n\".join(tok[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the lexicon content (first 10 entries)\n",
    "with open(path_lm_lexicon, 'r') as f:\n",
    "    lex = f.read().splitlines()\n",
    "print(\"\\n\".join(lex[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the line(s) corresponding to a word in the lexicon, you can use the following:\n",
    "w = 'hello'\n",
    "lex_w = [line for line in open(path_lm_lexicon) if line.startswith(w + '\\t')] # it's a list since there could be different pronunciation for the same word\n",
    "print(lex_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the CTC decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=path_lm_lexicon,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "\n",
    "# Apply the decoder to the `emission` tensor, and get the first element (batch_size=1) and best hypothesis\n",
    "ctc_decoder_result = decoder(emission)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The decoder output `ctc_decoder_result` contains many fields, including the predicted token IDs, and a `.words` field that contains the transcript as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token IDs using the .tokens field\n",
    "ctc_decoder_indices = ctc_decoder_result.tokens\n",
    "print(f\"Token indices: {ctc_decoder_indices}\")\n",
    "\n",
    "# Display the transript using the .words field\n",
    "print(f\"Words: {ctc_decoder_result.words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the token IDs using the .tokens field\n",
    "ctc_decoder_indices = ctc_decoder_result.tokens\n",
    "print(f\"Token indices: {ctc_decoder_indices}\")\n",
    "\n",
    "# Get the words vis the .words field\n",
    "print(f\"Words: {ctc_decoder_result.words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy decoder\n",
    "\n",
    "The greedy decoder we have seen in the first lab is a particular case of the CTC decoder when no langage model / lexicon is provided. It can be constructed by simply passing `None` as corresponding input arguments in the `ctc_decoder` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Construct a greedy decoder (no LM / lexicon), and apply it to the emission matrix\n",
    "decoder_nolm = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=None\n",
    ")\n",
    "\n",
    "ctc_nolm_result = decoder_nolm(emission)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since no language model is provided, the .words field returns an empty list\n",
    "print(ctc_nolm_result.words)\n",
    "\n",
    "# Then we have to manually convert token IDs to tokens using the decoder.idxs_to_tokens method\n",
    "# (+ a bit of postprocessing)\n",
    "ctc_nolm_tokens = decoder.idxs_to_tokens(ctc_nolm_result.tokens)\n",
    "ctc_nolm_transcript = \"\".join(ctc_nolm_tokens).replace(\"|\", \" \").strip().split() \n",
    "ctc_nolm_transcript = [w.lower() for w in ctc_nolm_transcript]\n",
    "\n",
    "print(f\"No LM Transcript: {ctc_nolm_transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the language model\n",
    "\n",
    "The language model is also expected to have a strong impact onto performance, since it guides the decoder towards more likely word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 1**</span>. Compare the CTC decoder using `librispeech-4-gram` files (lexicon, token, and language model downloaded above in this script) and the greedy decoder. To that end, for each decoder, perform ASR on the whole dataset (feel free to reuse/adapt code from the previous lab) and compute the mean WER. Can you interprete the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory, pattern='*.wav'):\n",
    "    \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(filename)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=True, max_files=None):\n",
    "\n",
    "    # Get the list of files in the dataset folder\n",
    "    audio_files = find_files(data_speech_dir)\n",
    "\n",
    "    # Take a subset of files\n",
    "    nfiles = len(audio_files)\n",
    "    if max_files:\n",
    "        nfiles = min(nfiles, max_files)\n",
    "    audio_files = audio_files[:nfiles]\n",
    "\n",
    "    # Initialize lists containing true and estimated transcripts, as well as WER\n",
    "    true_transcript_all = []\n",
    "    est_transcript_all = []\n",
    "    wer_all = []\n",
    "\n",
    "    for iaf, audio_file in enumerate(audio_files):\n",
    "        \n",
    "        # Get files path\n",
    "        audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "        transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "\n",
    "        # Load an audio signal\n",
    "        waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "\n",
    "        # Apply acoustic model and decoder\n",
    "        with torch.inference_mode():\n",
    "            emission, _ = acoustic_model(waveform)\n",
    "            ctc_decoder_results = decoder(emission)[0][0]\n",
    "\n",
    "        ctc_tokens = decoder.idxs_to_tokens(ctc_decoder_results.tokens)\n",
    "        est_transcript = \"\".join(ctc_tokens).replace(\"|\", \" \").strip().split()\n",
    "        est_transcript = [w.lower() for w in est_transcript]\n",
    "\n",
    "        # Load the true transcription\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        \n",
    "        # Compute WER\n",
    "        wer = get_wer(true_transcript, est_transcript)\n",
    "        wer_all.append(wer)\n",
    "\n",
    "        est_transcript_all.append(est_transcript)\n",
    "        true_transcript_all.append(true_transcript)\n",
    "        \n",
    "        # Display results\n",
    "        if verbose:\n",
    "            print(f\"File {iaf+1} / {nfiles}\")\n",
    "            print('Estimated transcript: ', est_transcript)\n",
    "            print('True transcript: ', true_transcript)\n",
    "            print(f\"WER: {wer*100:.1f} %\")\n",
    "\n",
    "    wer_mean = torch.FloatTensor(wer_all).mean()\n",
    "\n",
    "    return wer_mean, est_transcript_all, true_transcript_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC decoder with the 4-gram language model\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=path_lm_lexicon,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"LM: --- WER: {wer_mean*100:.1f} %\")\n",
    "\n",
    "# Greedy decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=labels,\n",
    "    lm=None,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"Greedy Decoder --- WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the lexicon\n",
    "\n",
    "The lexicon is expected to have a strong influence on ASR performance, since it constrains the decoder to produce only words that belong to this lexicon, therefore avoiding to procude words that potentially do not exist in a language or given corpus. Here, we propose to use a custom lexicon that only contains words that are in the dataset.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Perform ASR using such a custom lexicon. To that end:\n",
    "- Build a vocabulary (list of words) from the transcript files in the dataset (load the transcripts and remove duplicates).\n",
    "- Filter the downloaded lexicon by only keeping words from the vocabulary. Write this custom lexicon as a `.txt` file.\n",
    "- Create a dedocer that uses the 4-gram language model and this custom lexicon. Perform ASR on the dataset / compute the WER. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction to get a flat list from a list of list\n",
    "def flatten_list(list_of_lists):\n",
    "    return [x for L in list_of_lists for x in L]\n",
    "\n",
    "# Read all the transcripts in the dataset\n",
    "transcr_files = find_files(data_transc_dir, pattern='*.txt')\n",
    "true_transcript = []\n",
    "for f in transcr_files:\n",
    "    transc_file_path = os.path.join(data_transc_dir, f)\n",
    "    true_transcript.append(get_true_transcript(transc_file_path))\n",
    "\n",
    "# Flatten the list and remove duplicates\n",
    "vocab_dataset = list(set(flatten_list(true_transcript)))\n",
    "print('Words in the dataset:', len(vocab_dataset))\n",
    "\n",
    "# Filter the provided lexicon by keeping only words in our dataset\n",
    "custom_lexicon = []\n",
    "for w in vocab_dataset:\n",
    "    wl = [line for line in open(files.lexicon) if line.startswith(w + '\\t')]\n",
    "    custom_lexicon.append(wl)\n",
    "\n",
    "# again, flatten it\n",
    "custom_lexicon = flatten_list(custom_lexicon)\n",
    "\n",
    "# There are less entries in the lexicon than in our list of words from the dataset: several words are indeed not registered in the lexicon (we could add them manually)\n",
    "print('Entries in the custom lexicon: ', len(custom_lexicon))\n",
    "\n",
    "# Record this lexicon\n",
    "with open(\"mylexicon.txt\", \"w\") as f:\n",
    "    f.writelines(custom_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC decoder with the language model + a custom lexicon\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=\"mylexicon.txt\",\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"Custom lexicon --- WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: This custom lexicon produces a better transcript (lower WER) than the more general lexicon. This makes sense since our custom lexicon is specifically tailored for this dataset, and then words outside the dataset cannot be predicted. Nonetheless there are still some errors: besides the acoustic model, these can be due to several words in the dataset not being actually in the original downloaded lexicon, therefore the model cannot predict them using solely the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search parameters\n",
    "\n",
    "The beam search algorithm used in the CTC decoder depends on other parameters, such as `nbest` which determines the number of hypotheses to return, or `lm_weight` which adjust the relative importance of the language model vs. the acoustic model predictions. Here we only focus on `beam_size`, which determines the maximum number of best hypotheses to hold after each decoding step. Using larger beam sizes allows for exploring a larger range of possible hypotheses which can produce hypotheses with higher scores, which really is [the core](https://distill.pub/2017/ctc/) of the beam search algorithm.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 3**</span>. Perform ASR on the whole dataset folder for several values of the beam search size parameter: `beam_size` $\\in [1, 10, 100]$ (use the original downloaded lexicon, not the custom one). Compute the WER and the computation time (e.g., via the [time](https://docs.python.org/3/library/time.html#time.time) package). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "beam_sizes = [1, 10, 100]\n",
    "\n",
    "for beam_size in beam_sizes:\n",
    "    decoder = ctc_decoder(\n",
    "        lexicon=path_lm_lexicon,\n",
    "        tokens=path_lm_tokens,\n",
    "        lm=path_lm,\n",
    "        beam_size=beam_size,\n",
    "    )\n",
    "\n",
    "    time_start = time.time()\n",
    "    wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "    time_ellapsed = time.time() - time_start\n",
    "\n",
    "    print(f\"Beam search size: {beam_size} --- WER: {wer_mean*100:.1f} % --- Time: {time_ellapsed:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Using a beam search size that is too large (i.e., building too many hypothesis when decoding) does not improve performance compared to a lower value, but increases the computational cost. Thus, it is recommended to adjust this value in order to find a tradeoff between accuracy and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation / alignment\n",
    "\n",
    "**Unfinished / non working : can be changed to add an extra exercice if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctc_decoder_result.timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = {}\n",
    "word['start'] = 0\n",
    "word['end'] = 10\n",
    "word['label'] = 'l'\n",
    "word_segments = []\n",
    "word_segments.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: faire coder le time end (en utilisant '|' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignments_wav(waveform, emission, tokens, timesteps, sample_rate):\n",
    "    waveform /= waveform.max()\n",
    "    t = torch.arange(waveform.size(0)) / sample_rate\n",
    "    ratio = waveform.size(0) / emission.size(1) / sample_rate\n",
    "\n",
    "    chars = []\n",
    "    words = []\n",
    "    word_start = None\n",
    "    for token, timestep in zip(tokens, timesteps * ratio):\n",
    "        if token == \"|\":\n",
    "            if word_start is not None:\n",
    "                words.append((word_start, timestep))\n",
    "            word_start = None\n",
    "        else:\n",
    "            chars.append((token, timestep))\n",
    "            if word_start is None:\n",
    "                word_start = timestep\n",
    "\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.plot(t, waveform)\n",
    "    for token, timestep in chars:\n",
    "        plt.annotate(token, (timestep, 1))\n",
    "    for word_start, word_end in words:\n",
    "        plt.axvspan(word_start, word_end, alpha=0.1, color=\"red\")\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ctc_nolm_tokens = decoder.idxs_to_tokens(ctc_nolm_result.tokens)\n",
    "plot_alignments_wav(waveform[0], emission, ctc_nolm_tokens, ctc_decoder_result.timesteps, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignments(segments, word_segments, waveform, sample_rate=bundle.sample_rate):\n",
    "\n",
    "    # The original waveform\n",
    "    ratio = waveform.size(0) / sample_rate / trellis.size(0)\n",
    "    ax2.specgram(waveform, Fs=sample_rate)\n",
    "    for word in word_segments:\n",
    "        x0 = ratio * word.start\n",
    "        x1 = ratio * word.end\n",
    "        ax2.axvspan(x0, x1, facecolor=\"none\", edgecolor=\"white\", hatch=\"/\")\n",
    "        ax2.annotate(f\"{word.score:.2f}\", (x0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "    for seg in segments:\n",
    "        if seg.label != \"|\":\n",
    "            ax2.annotate(seg.label, (seg.start * ratio, sample_rate * 0.55), annotation_clip=False)\n",
    "    ax2.set_xlabel(\"time [second]\")\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "plot_alignments(\n",
    "    segments,\n",
    "    word_segments,\n",
    "    waveform[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_segment(waveform, word, num_time_steps, sr=16000):\n",
    "    ratio = waveform.shape[-1] / num_time_steps\n",
    "    x0 = int(ratio * word['start'])\n",
    "    x1 = int(ratio * word['end'])\n",
    "    print(f\"Character: {word['label']}\")\n",
    "    segment = waveform[:, x0:x1]\n",
    "    return IPython.display.Audio(segment, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_segments[0]\n",
    "display_segment(waveform, word, num_time_steps, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
