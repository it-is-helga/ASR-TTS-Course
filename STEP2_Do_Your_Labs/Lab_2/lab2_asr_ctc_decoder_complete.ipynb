{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Begin\n",
    "\n",
    "This is a practical part of your ASR-TTS course. In total you will have 5 labs. Three of which will be focused on Automatic Speech Recognition and two on Text-to-Speech models. Each lab will last two hours and consist of two parts:\n",
    "* Reading Part\n",
    "* Coding Part \n",
    "\n",
    "In each part you might find question or tasks/activities to complete. The grading of the labs is explained below.\n",
    "\n",
    "LAB 2/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will you learn in LAB 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How feature extraction works in practice, including time--frequency representations and their parameters.\n",
    "* How different decoder strategies (e.g. greedy decoding. beam search...) and their parameters affect ASR outputs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition: Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Read the following, remember and explain how feature extraction works: \n",
    "\n",
    "https://thesai.org/Downloads/Volume12No8/Paper_21-Automatic_Speech_Recognition_Features_Extraction_Techniques.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw audio is a 1D waveform: amplitude over time. But ASR models usually don’t work directly on raw samples; they rely on features that make speech patterns easier to learn. Today we’ll build the most common ones: spectrograms, log-mel spectrograms, and MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An audio waveform is the most basic representation of sound in a computer.\n",
    "When we record speech, the microphone converts air pressure changes into an electrical signal, and the computer stores this signal as a sequence of numbers over time.\n",
    "\n",
    "Each number represents: how much the air pressure deviates from silence at a specific moment in time\n",
    "\n",
    "So a waveform is simply: amplitude (loudness) as a function of time\n",
    "\n",
    "Unlike humans, a computer does not listen or understand meaning.\n",
    "It only sees numbers. A waveform looks like this to a model:\n",
    "\n",
    "[0.002, 0.01, -0.03, -0.01, 0.0, 0.02, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Rate\n",
    "\n",
    "Speech is stored by sampling the signal many times per second. Example: 16 kHz = 16,000 samples per second every second of speech → 16,000 numbers. Higher sampling rate means more detail, larger files and more computation. Most ASR systems use 16 kHz because it captures speech frequencies well while remaining efficient. While waveforms contain all information, they are not convenient for learning speech patterns directly: \n",
    "\n",
    "* speech information is spread over time\n",
    "* frequency content (pitch, formants) is hidden\n",
    "* small shifts in time change the signal a lot\n",
    "\n",
    "That’s why ASR systems transform waveforms into time–frequency features, such as spectrograms, Mel spectrograms, MFCCs.\n",
    "\n",
    "➡️ This is the goal of feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "from speechbrain.processing.features import STFT\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the code from Lab 1 load one audio file and plot a waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discrete Fourier Transform (DFT) \n",
    "\n",
    "DFT is a method used by computers to analyze the frequency content of a digital audio signal. A speech recording is stored as a sequence of sampled values, and the DFT processes a finite segment of these values to determine which frequencies are present and how strong they are. The result of the DFT is a frequency-domain representation, where each frequency component corresponds to a specific frequency and an associated magnitude. In speech processing, the DFT provides a fundamental way to describe the spectral content of a signal, allowing systems to move from a time-based representation of audio to a frequency-based one, which is essential for understanding and modeling speech sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Read the documentation here and perform DFT on one audio: https://speechbrain.readthedocs.io/en/stable/tutorials/preprocessing/fourier-transform-and-spectrograms.html#short-term-fourier-transform-stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the magnitude and phase of the DFT and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Time Fourier Transform (STFT) Spectrogram — Linear Frequency Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech changes very quickly over time, so instead of analyzing the whole sound at once, we analyze it little by little. The audio signal is first cut into short pieces called frames, each lasting only a few milliseconds (typically about 25 ms), which is short enough that speech is almost stable inside each frame. These frames overlap slightly so that no information is lost at the borders and transitions remain smooth. Before analyzing each frame, we apply a window function, most commonly the Hann window, which gently fades the signal at the beginning and end of the frame to avoid artificial frequencies caused by sharp cuts. Once a frame is short, overlapping, and smoothly windowed, the computer analyzes which frequencies are present and how strong they are. By stacking the frequency information from all frames over time, we obtain a spectrogram, where:\n",
    "* the horizontal axis represents time,\n",
    "* the vertical axis represents frequency (evenly spaced in Hz, called linear frequency),\n",
    "* and the color represents energy.\n",
    "\n",
    "This process is called the Short-Time Fourier Transform (STFT) because we transform short pieces of sound into frequencies, revealing how speech frequencies evolve over time.\n",
    "\n",
    "In summary, the STFT works by:\n",
    "* splitting speech into short frames (e.g. 25 ms),\n",
    "* applying a window (Hann),\n",
    "* applying a Fourier Transform to each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "\n",
    "Read the documentation here and perform STFT on one audio: https://speechbrain.readthedocs.io/en/stable/tutorials/preprocessing/fourier-transform-and-spectrograms.html#short-term-fourier-transform-stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the spectogram reading the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "As you can see, STFT has parameters, what are they and what do they represent? Feel free to google. Explain them below in your own EASY words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Experiment by changing the parameters and producing spectograms, tell me your observations, how do the spectpgrams change as you increase/decrease each parameter? I want to see the produced spectograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Mel Spectrogram (typical neural ASR feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mel scale is a perceptual frequency scale designed to reflect how humans hear sound rather than how frequencies are spaced mathematically. In human hearing, we are much more sensitive to small changes at low frequencies than at high frequencies, whereas the standard frequency scale (in Hertz) treats all frequency intervals equally. The Mel scale compresses high frequencies and provides finer resolution at low frequencies, making it especially suitable for speech analysis, where important information such as pitch and formants lies mostly in the lower frequency range. In ASR feature extraction, applying the Mel scale means grouping and weighting frequencies in a way that better matches human perception, which leads to features that are more robust and informative for modeling speech sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs (Mel-Frequency Cepstral Coefficients) are a compact representation of speech that describe the overall shape of the speech spectrum in a way that is well suited for modeling and recognition. They are computed by first converting the speech signal into a Mel-scaled, log-energy spectrum, and then applying a mathematical transform that summarizes this spectrum into a small number of coefficients. This final step reduces redundancy and emphasizes the most important spectral patterns while discarding fine details that are less relevant for recognition. As a result, MFCCs provide a low-dimensional, stable description of speech and have been widely used as features in traditional ASR systems and early neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Read the documentation and produce MFCC plot: https://speechbrain.readthedocs.io/en/stable/tutorials/preprocessing/speech-features.html#mel-frequency-cepstral-coefficients-mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition: Decoders\n",
    "\n",
    "**Name**:\n",
    "\n",
    "The first lab introduced the building blocks of an ASR system, including feature extraction and classification with an acoustic model (wav2vec2), which produced an *emission* matrix (probability for each character at each time step). From this emission matrix, we could compute the most likely character at each time step using a naïve *greedy* decoder. The drawback of such an approach is the lack of context, which can produce sequences of characters that do not correspond to actual words, and/or sequences of words that are incorrect / do not correspond to any language rules.\n",
    "\n",
    "In this lab, we introduce and compare multiple decoding techniques. One of the more advanced techniques is based on [connectionist temporal classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c) (CTC). The general idea of such a decoder is to consider some context (sequences of characters, possible words, and possible sequences of words), in oder to yield more likely / realistic outputs than those given by the greedy decoder.\n",
    "\n",
    "<center><a href=\"https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XbIp4pn38rxL_DwzSeYVxQ.png\" width=\"400\"></a></center>\n",
    "\n",
    "To do so, the CTC decoder relies on three main components:\n",
    "    \n",
    "- A **beam search**, which is an algorithm to efficently find the *best path* from the emission matrix, that is, the sequence of characters with highest probability (rather than the sequence of individually most likely characters).\n",
    "- A **lexicon**, which is a mapping between token sequences (list of characters) and words. It is used to restrict the search space of the decoder to words that only belong to this dictionary (e.g., the word \"azfpojazflj\" does not exist in the English vocabulary).\n",
    "- A **language model**, which specifies sequences of words that are more likely to occur than others. A common choice of language model is an $n$-gram, which is a statistical model for the probability of occurrence of a given word based on the previous $n$ ones (for instance, the sequence \"the sky is\" is more likely to be followed with \"blue\" rather than \"trumpet\").\n",
    "\n",
    "The CTC decoder combines these ingredients to compute the score of several word sequences (or *hypothesis*), in order to find the best possible transcript. In this lab, we study the influence of the lexicon, language model, and the beam search size onto ASR performance from a practical perspective, without going into the technical details of the [beam search algorithm](https://www.width.ai/post/what-is-beam-search) or the [CTC loss](https://distill.pub/2017/ctc/), which can also be used for training the network (as we will see in lab 3).\n",
    "\n",
    "**Note**: This lab is based on this [tutorial](https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html), which you can check for more details on CTC decoder parameters in torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.models.decoder import ctc_decoder, download_pretrained_files\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "torch.random.manual_seed(0)\n",
    "MAX_FILES = 2 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = \"/home/aine/Teaching/ASR-TTS-Course/asr-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "As in the previous lab, we first load an example speech signal, and we display it. We also provide the function to get the true transcript and compute the WER. Finally, we load the wav2vec2 acoustic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the function for loading the true transcript and computing the WER\n",
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().split()\n",
    "    return true_transcript\n",
    "\n",
    "def get_wer(true_transcript, est_transcript):\n",
    "    wer = torchaudio.functional.edit_distance(true_transcript, est_transcript) / len(true_transcript)\n",
    "    return wer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load an accoustic model for our ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Remember and explain again what does an accoustic model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "# Apply the model to the waveform to get the emission tensor\n",
    "with torch.inference_mode():\n",
    "    emission, _ = acoustic_model(waveform)\n",
    "    num_time_steps = emission.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "We are trying to run an ASR now, however, we are yet to see any steps for feature extraction. Where does feature extraction happen and why can't you see it? https://docs.pytorch.org/audio/main/tutorials/speech_recognition_pipeline_tutorial.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "What is the purpose of a decoder in your own words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Decoder\n",
    "\n",
    "The CTC decoder can be constructed directly by using the `ctc_decoder` function in torchaudio. In addition to the parameters related to the beam search (we will study them later on), it takes as inputs:\n",
    "- the list of tokens, in order to map emissions to characters in the classifier.\n",
    "- the path to the lexicon, expected as a .txt file containing, on each line, a word followed by its space-split tokens (and special end-of-sequence token `|`):\n",
    "\n",
    "```\n",
    "# lexicon.txt\n",
    "a     a |\n",
    "able  a b l e |\n",
    "about a b o u t |\n",
    "...\n",
    "```\n",
    "- the path to the language model, expected as a .bin file.\n",
    "\n",
    "All these are assembled in pretrained files that can be downloaded using the `download_pretrained_files` function (this might take some time as the language model can be large), and then used to contruct the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the files corresponding to (pretrained) language model, which comes with lexicon and tokens\n",
    "files = download_pretrained_files(\"librispeech\")\n",
    "path_lm_tokens = files.tokens\n",
    "path_lm_lexicon = files.lexicon\n",
    "path_lm = files.lm\n",
    "\n",
    "print(path_lm_tokens)\n",
    "print(path_lm_lexicon)\n",
    "print(path_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "|\n",
      "e\n",
      "t\n",
      "a\n",
      "o\n",
      "n\n",
      "i\n",
      "h\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# Vizualize the first 10 tokens (includes the blank and end-of-word token)\n",
    "with open(path_lm_tokens, 'r') as f:\n",
    "    tok = f.read().splitlines()\n",
    "print(\"\\n\".join(tok[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "What is the purpose of the lexicon here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\ta |\n",
      "a''s\ta ' ' s |\n",
      "a'body\ta ' b o d y |\n",
      "a'court\ta ' c o u r t |\n",
      "a'd\ta ' d |\n",
      "a'gha\ta ' g h a |\n",
      "a'goin\ta ' g o i n |\n",
      "a'll\ta ' l l |\n",
      "a'm\ta ' m |\n",
      "a'mighty\ta ' m i g h t y |\n"
     ]
    }
   ],
   "source": [
    "# Vizualize the lexicon content (first 10 entries)\n",
    "with open(path_lm_lexicon, 'r') as f:\n",
    "    lex = f.read().splitlines()\n",
    "print(\"\\n\".join(lex[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello\\th e l l o |\\n']\n"
     ]
    }
   ],
   "source": [
    "# To obtain the line(s) corresponding to a word in the lexicon, you can use the following:\n",
    "w = 'hello'\n",
    "lex_w = [line for line in open(path_lm_lexicon) if line.startswith(w + '\\t')] # it's a list since there could be different pronunciation for the same word\n",
    "print(lex_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the CTC decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=path_lm_lexicon,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "\n",
    "# Apply the decoder to the `emission` tensor, and get the first element (batch_size=1) and best hypothesis\n",
    "ctc_decoder_result = decoder(emission)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The decoder output `ctc_decoder_result` contains many fields, including the predicted token IDs, and a `.words` field that contains the transcript as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: tensor([ 1, 18,  7, 22,  2,  1,  6,  5,  3,  1,  9,  5,  1,  2,  4, 10,  6,  2,\n",
      "         9,  3,  1,  4,  1, 14,  7,  6, 11,  1,  3,  5,  1,  3,  8,  2,  9,  2,\n",
      "         1, 14,  2, 14,  5, 10,  7,  2,  9,  1, 16,  8,  7, 12, 11,  1,  1])\n",
      "Words: ['give', 'not', 'so', 'earnest', 'a', 'mind', 'to', 'these', 'memories', 'child']\n"
     ]
    }
   ],
   "source": [
    "# Get the token IDs using the .tokens field\n",
    "ctc_decoder_indices = ctc_decoder_result.tokens\n",
    "print(f\"Token indices: {ctc_decoder_indices}\")\n",
    "\n",
    "# Display the transript using the .words field\n",
    "print(f\"Words: {ctc_decoder_result.words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy decoder\n",
    "\n",
    "The greedy decoder we have seen in the first lab is a particular case of the CTC decoder when no langage model / lexicon is provided. It can be constructed by simply passing `None` as corresponding input arguments in the `ctc_decoder` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Construct a greedy decoder (no LM / lexicon), and apply it to the emission matrix\n",
    "decoder_nolm = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=None\n",
    ")\n",
    "\n",
    "ctc_nolm_result = decoder_nolm(emission)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No LM Transcript: ['give', 'not', 'so', 'earnest', 'a', 'mind', 'to', 'these', 'mumories', 'child']\n"
     ]
    }
   ],
   "source": [
    "# Since no language model is provided, the .words field returns an empty list\n",
    "print(ctc_nolm_result.words)\n",
    "\n",
    "# Then we have to manually convert token IDs to tokens using the decoder.idxs_to_tokens method\n",
    "# (+ a bit of postprocessing)\n",
    "ctc_nolm_tokens = decoder.idxs_to_tokens(ctc_nolm_result.tokens)\n",
    "ctc_nolm_transcript = \"\".join(ctc_nolm_tokens).replace(\"|\", \" \").strip().split() \n",
    "ctc_nolm_transcript = [w.lower() for w in ctc_nolm_transcript]\n",
    "\n",
    "print(f\"No LM Transcript: {ctc_nolm_transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search vs Greedy decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Explain how the Greedy and Beam decoders work: https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Time to try coding it yourself. Refering to the codes in lab 1 and this lab, run an asr using your chosen wav2vec accoustic model with both of these decoders on 10 audio files and compare the average WERs. Comment on the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the language model\n",
    "\n",
    "The language model is also expected to have a strong impact onto performance, since it guides the decoder towards more likely word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 1**</span>. Compare the CTC decoder using `librispeech-4-gram` files (lexicon, token, and language model downloaded above in this script) and the greedy decoder. To that end, for each decoder, perform ASR on the whole dataset (feel free to reuse/adapt code from the previous lab) and compute the mean WER. Can you interprete the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory, pattern='*.wav'):\n",
    "    \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(filename)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=True, max_files=None):\n",
    "\n",
    "    # Get the list of files in the dataset folder\n",
    "    audio_files = find_files(data_speech_dir)\n",
    "\n",
    "    # Take a subset of files\n",
    "    nfiles = len(audio_files)\n",
    "    if max_files:\n",
    "        nfiles = min(nfiles, max_files)\n",
    "    audio_files = audio_files[:nfiles]\n",
    "\n",
    "    # Initialize lists containing true and estimated transcripts, as well as WER\n",
    "    true_transcript_all = []\n",
    "    est_transcript_all = []\n",
    "    wer_all = []\n",
    "\n",
    "    for iaf, audio_file in enumerate(audio_files):\n",
    "        \n",
    "        # Get files path\n",
    "        audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "        transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "\n",
    "        # Load an audio signal\n",
    "        waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "\n",
    "        # Apply acoustic model and decoder\n",
    "        with torch.inference_mode():\n",
    "            emission, _ = acoustic_model(waveform)\n",
    "            ctc_decoder_results = decoder(emission)[0][0]\n",
    "\n",
    "        ctc_tokens = decoder.idxs_to_tokens(ctc_decoder_results.tokens)\n",
    "        est_transcript = \"\".join(ctc_tokens).replace(\"|\", \" \").strip().split()\n",
    "        est_transcript = [w.lower() for w in est_transcript]\n",
    "\n",
    "        # Load the true transcription\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        \n",
    "        # Compute WER\n",
    "        wer = get_wer(true_transcript, est_transcript)\n",
    "        wer_all.append(wer)\n",
    "\n",
    "        est_transcript_all.append(est_transcript)\n",
    "        true_transcript_all.append(true_transcript)\n",
    "        \n",
    "        # Display results\n",
    "        if verbose:\n",
    "            print(f\"File {iaf+1} / {nfiles}\")\n",
    "            print('Estimated transcript: ', est_transcript)\n",
    "            print('True transcript: ', true_transcript)\n",
    "            print(f\"WER: {wer*100:.1f} %\")\n",
    "\n",
    "    wer_mean = torch.FloatTensor(wer_all).mean()\n",
    "\n",
    "    return wer_mean, est_transcript_all, true_transcript_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC decoder with the 4-gram language model\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=path_lm_lexicon,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"LM: --- WER: {wer_mean*100:.1f} %\")\n",
    "\n",
    "# Greedy decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=labels,\n",
    "    lm=None,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"Greedy Decoder --- WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the lexicon\n",
    "\n",
    "The lexicon is expected to have a strong influence on ASR performance, since it constrains the decoder to produce only words that belong to this lexicon, therefore avoiding to procude words that potentially do not exist in a language or given corpus. Here, we propose to use a custom lexicon that only contains words that are in the dataset.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Perform ASR using such a custom lexicon. To that end:\n",
    "- Build a vocabulary (list of words) from the transcript files in the dataset (load the transcripts and remove duplicates).\n",
    "- Filter the downloaded lexicon by only keeping words from the vocabulary. Write this custom lexicon as a `.txt` file.\n",
    "- Create a dedocer that uses the 4-gram language model and this custom lexicon. Perform ASR on the dataset / compute the WER. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction to get a flat list from a list of list\n",
    "def flatten_list(list_of_lists):\n",
    "    return [x for L in list_of_lists for x in L]\n",
    "\n",
    "# Read all the transcripts in the dataset\n",
    "transcr_files = find_files(data_transc_dir, pattern='*.txt')\n",
    "true_transcript = []\n",
    "for f in transcr_files:\n",
    "    transc_file_path = os.path.join(data_transc_dir, f)\n",
    "    true_transcript.append(get_true_transcript(transc_file_path))\n",
    "\n",
    "# Flatten the list and remove duplicates\n",
    "vocab_dataset = list(set(flatten_list(true_transcript)))\n",
    "print('Words in the dataset:', len(vocab_dataset))\n",
    "\n",
    "# Filter the provided lexicon by keeping only words in our dataset\n",
    "custom_lexicon = []\n",
    "for w in vocab_dataset:\n",
    "    wl = [line for line in open(files.lexicon) if line.startswith(w + '\\t')]\n",
    "    custom_lexicon.append(wl)\n",
    "\n",
    "# again, flatten it\n",
    "custom_lexicon = flatten_list(custom_lexicon)\n",
    "\n",
    "# There are less entries in the lexicon than in our list of words from the dataset: several words are indeed not registered in the lexicon (we could add them manually)\n",
    "print('Entries in the custom lexicon: ', len(custom_lexicon))\n",
    "\n",
    "# Record this lexicon\n",
    "with open(\"mylexicon.txt\", \"w\") as f:\n",
    "    f.writelines(custom_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC decoder with the language model + a custom lexicon\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=\"mylexicon.txt\",\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"Custom lexicon --- WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "How does this custom lexicon compare to the one we used before? How does it affect WER and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search parameters\n",
    "\n",
    "The beam search algorithm used in the CTC decoder depends on other parameters, such as `nbest` which determines the number of hypotheses to return, or `lm_weight` which adjust the relative importance of the language model vs. the acoustic model predictions. Here we only focus on `beam_size`, which determines the maximum number of best hypotheses to hold after each decoding step. Using larger beam sizes allows for exploring a larger range of possible hypotheses which can produce hypotheses with higher scores, which really is [the core](https://distill.pub/2017/ctc/) of the beam search algorithm.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 3**</span>. Perform ASR on the whole dataset folder for several values of the beam search size parameter: `beam_size` $\\in [1, 10, 100]$ (use the original downloaded lexicon, not the custom one). Compute the WER and the computation time (e.g., via the [time](https://docs.python.org/3/library/time.html#time.time) package). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "beam_sizes = [1, 10, 100]\n",
    "\n",
    "for beam_size in beam_sizes:\n",
    "    decoder = ctc_decoder(\n",
    "        lexicon=path_lm_lexicon,\n",
    "        tokens=path_lm_tokens,\n",
    "        lm=path_lm,\n",
    "        beam_size=beam_size,\n",
    "    )\n",
    "\n",
    "    time_start = time.time()\n",
    "    wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "    time_ellapsed = time.time() - time_start\n",
    "\n",
    "    print(f\"Beam search size: {beam_size} --- WER: {wer_mean*100:.1f} % --- Time: {time_ellapsed:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Compare the performance using different beam sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "What other parameters could we tweak and what are they responsible for? Try tweaking them, how does it affect WER? \n",
    "https://docs.pytorch.org/audio/2.8/tutorials/asr_inference_with_ctc_decoder_tutorial.html#beam-search-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding blocks and questions that will be tested next class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT\n",
    "\n",
    "signal = read_audio('').unsqueeze(0) \n",
    "# Why do we need to unsqueeze it???\n",
    "\n",
    "compute_STFT = STFT(sample_rate=16000, win_length=25, hop_length=10, n_fft=400) # 25 ms, 10 ms\n",
    "signal_STFT = compute_STFT(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = signal_STFT.pow(2).sum(-1) # power spectrogram\n",
    "spectrogram = spectrogram.squeeze(0).transpose(0,1)\n",
    "\n",
    "spectrogram_log = torch.log(spectrogram) # for graphical convenience\n",
    "\n",
    "plt.imshow(spectrogram_log.squeeze(0), cmap='hot', interpolation='nearest', origin='lower')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the difference between waveform and spectrogram?\n",
    "* what is pseudo-likelihood and how is it obtained? (Google or refer to your lecture slides)\n",
    "* What is the STFT and why is it used for speech?\n",
    "* What do n_fft, window length, and hop length control?\n",
    "* What is the difference between log-Mel features and MFCCs?\n",
    "* Is pitch a relevant acoustic feature for voice command task in Chinese language? Why? What about French?\n",
    "* What is the benefit of the TDNN architecture compared to a conventional CNN? Explain both architectures. (You will need to google for this one or refer to your lecture slides)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Question with a bit of context:\n",
    "Conceptually, a decoder objective is to search for the most likely sequence of words:\n",
    "\n",
    "![Decoder objective](fig1.png)\n",
    "\n",
    "https://jonathan-hui.medium.com/speech-recognition-asr-decoding-f152aebed779\n",
    "\n",
    "The formula above means:\n",
    "“among all possible sentences, choose the one that has the highest probability given the audio.”\n",
    "\n",
    "Now look at the formula below, given that the left side represents the probability of a sentence given the audio, what does it mean? What does the sign between them mean?\n",
    "\n",
    "![Decoder decomposition](fig2.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
